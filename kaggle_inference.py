#!/usr/bin/env python3
"""
Kaggleä¼˜åŒ–ç‰ˆæ¨ç†è„šæœ¬
é€‚é…Kaggleç¯å¢ƒçš„MOSS-TTSD-KAGGLEæ¨ç†
"""

import json
import torch
import torchaudio
import accelerate
import argparse
import os
import warnings
import gc
from pathlib import Path

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from generation_utils import load_model, process_batch
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿æ‰€æœ‰é¡¹ç›®æ–‡ä»¶éƒ½åœ¨å½“å‰ç›®å½•ä¸­")
    exit(1)

# Kaggleç¯å¢ƒé…ç½®
KAGGLE_CONFIG = {
    "MODEL_PATH": "fnlp/MOSS-TTSD-v0.5",
    "SYSTEM_PROMPT": "You are a speech synthesizer that generates natural, realistic, and human-like conversational audio from dialogue text.",
    "SPT_CONFIG_PATH": "XY_Tokenizer/config/xy_tokenizer_config.yaml",
    "SPT_CHECKPOINT_PATH": "XY_Tokenizer/weights/xy_tokenizer.ckpt",
    "MAX_CHANNELS": 8,
    "DEFAULT_OUTPUT_DIR": "outputs",
    "TEMP_DIR": "temp_audio"
}

def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥Kaggleç¯å¢ƒ...")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        return True
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        return False

def optimize_for_kaggle():
    """Kaggleç¯å¢ƒä¼˜åŒ–"""
    print("âš¡ ä¼˜åŒ–Kaggleç¯å¢ƒ...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
    
    # ä¼˜åŒ–PyTorchè®¾ç½®
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    for dir_name in [KAGGLE_CONFIG["DEFAULT_OUTPUT_DIR"], KAGGLE_CONFIG["TEMP_DIR"]]:
        os.makedirs(dir_name, exist_ok=True)

def load_models_with_fallback():
    """åŠ è½½æ¨¡å‹ï¼ˆå¸¦é™çº§å¤„ç†ï¼‰"""
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # å°è¯•ä¸åŒçš„attentionå®ç°
    attention_implementations = ["flash_attention_2", "sdpa", "eager"]
    
    for attn_impl in attention_implementations:
        try:
            print(f"ğŸ”§ å°è¯•ä½¿ç”¨ {attn_impl} attention...")
            
            tokenizer, model, spt = load_model(
                KAGGLE_CONFIG["MODEL_PATH"], 
                KAGGLE_CONFIG["SPT_CONFIG_PATH"], 
                KAGGLE_CONFIG["SPT_CHECKPOINT_PATH"],
                torch_dtype=torch.bfloat16,
                attn_implementation=attn_impl
            )
            
            spt = spt.to(device)
            model = model.to(device)
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (ä½¿ç”¨ {attn_impl})")
            return tokenizer, model, spt, device, attn_impl
            
        except Exception as e:
            print(f"âŒ {attn_impl} å¤±è´¥: {e}")
            continue
    
    raise RuntimeError("æ‰€æœ‰attentionå®ç°éƒ½å¤±è´¥äº†")

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰è¾“å…¥æ–‡ä»¶ï¼‰"""
    sample_data = [
        {
            "text": "[S1]Hello, this is a test of the MOSS-TTSD-KAGGLE system.[S2]Yes, it sounds very natural and realistic![S1]I'm glad you think so. This is running on Kaggle with dual T4 GPUs."
        },
        {
            "text": "[S1]ä½ å¥½ï¼Œè¿™æ˜¯MOSS-TTSD-KAGGLEç³»ç»Ÿçš„æµ‹è¯•ã€‚[S2]æ˜¯çš„ï¼Œå¬èµ·æ¥éå¸¸è‡ªç„¶å’ŒçœŸå®ï¼[S1]å¾ˆé«˜å…´ä½ è¿™ä¹ˆè®¤ä¸ºã€‚è¿™æ˜¯åœ¨Kaggleä¸Šä½¿ç”¨åŒT4 GPUè¿è¡Œçš„ã€‚"
        }
    ]
    
    # ä¿å­˜ç¤ºä¾‹æ•°æ®
    sample_file = "kaggle_sample.jsonl"
    with open(sample_file, "w", encoding="utf-8") as f:
        for item in sample_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶: {sample_file}")
    return sample_file

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Kaggleä¼˜åŒ–ç‰ˆTTSæ¨ç†")
    parser.add_argument("--jsonl", default=None, help="è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", default=KAGGLE_CONFIG["DEFAULT_OUTPUT_DIR"], help="è¾“å‡ºç›®å½•")
    parser.add_argument("--use_normalize", action="store_true", default=True, help="ä½¿ç”¨æ–‡æœ¬è§„èŒƒåŒ–")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--max_samples", type=int, default=5, help="æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ˆKaggleé™åˆ¶ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨Kaggleç‰ˆMOSS-TTSD-KAGGLEæ¨ç†...")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    has_gpu = check_kaggle_environment()
    
    # 2. ä¼˜åŒ–ç¯å¢ƒ
    optimize_for_kaggle()
    
    # 3. å¤„ç†è¾“å…¥æ–‡ä»¶
    if args.jsonl is None or not os.path.exists(args.jsonl):
        print("âš ï¸ æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        args.jsonl = create_sample_data()
    
    # 4. åŠ è½½æ•°æ®
    try:
        with open(args.jsonl, "r", encoding="utf-8") as f:
            items = [json.loads(line.strip()) for line in f if line.strip()]
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆKaggleèµ„æºé™åˆ¶ï¼‰
        if len(items) > args.max_samples:
            print(f"âš ï¸ æ ·æœ¬æ•°é‡è¿‡å¤šï¼Œé™åˆ¶ä¸ºå‰{args.max_samples}ä¸ª")
            items = items[:args.max_samples]
        
        print(f"ğŸ“Š åŠ è½½äº† {len(items)} ä¸ªæ ·æœ¬")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 5. åŠ è½½æ¨¡å‹
    try:
        tokenizer, model, spt, device, attn_impl = load_models_with_fallback()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # 6. è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        accelerate.utils.set_seed(args.seed)
        print(f"ğŸ² è®¾ç½®éšæœºç§å­: {args.seed}")
    
    # 7. å¼€å§‹æ¨ç†
    print("ğŸµ å¼€å§‹éŸ³é¢‘ç”Ÿæˆ...")
    try:
        actual_texts_data, audio_results = process_batch(
            batch_items=items,
            tokenizer=tokenizer,
            model=model,
            spt=spt,
            device=device,
            system_prompt=KAGGLE_CONFIG["SYSTEM_PROMPT"],
            start_idx=0,
            use_normalize=args.use_normalize
        )
        
        # 8. ä¿å­˜ç»“æœ
        saved_count = 0
        results_info = []
        
        for idx, audio_result in enumerate(audio_results):
            if audio_result is not None:
                output_path = os.path.join(args.output_dir, f"kaggle_output_{idx}.wav")
                
                try:
                    torchaudio.save(
                        output_path,
                        audio_result["audio_data"],
                        audio_result["sample_rate"]
                    )
                    
                    # è®¡ç®—éŸ³é¢‘æ—¶é•¿
                    duration = audio_result["audio_data"].shape[-1] / audio_result["sample_rate"]
                    
                    results_info.append({
                        "index": idx,
                        "file": output_path,
                        "duration": f"{duration:.2f}s",
                        "sample_rate": audio_result["sample_rate"]
                    })
                    
                    print(f"âœ… ä¿å­˜éŸ³é¢‘ {idx}: {output_path} ({duration:.2f}s)")
                    saved_count += 1
                    
                except Exception as e:
                    print(f"âŒ ä¿å­˜éŸ³é¢‘ {idx} å¤±è´¥: {e}")
            else:
                print(f"âš ï¸ è·³è¿‡æ ·æœ¬ {idx}ï¼ˆç”Ÿæˆå¤±è´¥ï¼‰")
        
        # 9. ç”Ÿæˆç»“æœæŠ¥å‘Š
        report_path = os.path.join(args.output_dir, "kaggle_results.json")
        report = {
            "total_samples": len(items),
            "successful_generations": saved_count,
            "failed_generations": len(items) - saved_count,
            "model_info": {
                "model_path": KAGGLE_CONFIG["MODEL_PATH"],
                "attention_implementation": attn_impl,
                "device": device,
                "use_normalize": args.use_normalize
            },
            "results": results_info
        }
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ç»“æœæŠ¥å‘Šä¿å­˜è‡³: {report_path}")
        
        # 10. æ¸…ç†å†…å­˜
        del model, spt, tokenizer
        torch.cuda.empty_cache()
        gc.collect()
        
        print("=" * 60)
        print(f"ğŸ‰ æ¨ç†å®Œæˆï¼æˆåŠŸç”Ÿæˆ {saved_count}/{len(items)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        
    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
